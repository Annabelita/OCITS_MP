{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import argparse\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "from multiprocessing import Pool, Process, Value, Lock\n",
    "\n",
    "from requests.exceptions import ConnectionError, ReadTimeout, TooManyRedirects, MissingSchema, InvalidURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "pathlib.Path().absolute()\n",
    "#print(args.data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picked the following clases:\n",
      "['animal', 'plant']\n",
      "Scraping images for class \"animal\"\n",
      "Multiprocessing workers: 8\n",
      "\n",
      "Scraping stats:\n",
      "STATS For class is_flickr:\n",
      " tried 242.0 urls with 175.0 successes\n",
      "72.31404958677686% success rate for is_flickr urls \n",
      "0.09708028793334961 seconds spent per is_flickr succesful image download\n",
      "STATS For class not_flickr:\n",
      " tried 0.0 urls with 0.0 successes\n",
      "STATS For class all:\n",
      " tried 242.0 urls with 175.0 successes\n",
      "72.31404958677686% success rate for all urls \n",
      "0.09711029325212751 seconds spent per all succesful image download\n",
      "\n",
      "Scraping stats:\n",
      "STATS For class is_flickr:\n",
      " tried 492.0 urls with 354.0 successes\n",
      "71.95121951219512% success rate for is_flickr urls \n",
      "0.08186562317239363 seconds spent per is_flickr succesful image download\n",
      "STATS For class not_flickr:\n",
      " tried 0.0 urls with 0.0 successes\n",
      "STATS For class all:\n",
      " tried 492.0 urls with 354.0 successes\n",
      "71.95121951219512% success rate for all urls \n",
      "0.08187852875661042 seconds spent per all succesful image download\n",
      "\n",
      "Scraping stats:\n",
      "STATS For class is_flickr:\n",
      " tried 743.0 urls with 524.0 successes\n",
      "70.52489905787348% success rate for is_flickr urls \n",
      "0.07933066773960609 seconds spent per is_flickr succesful image download\n",
      "STATS For class not_flickr:\n",
      " tried 0.0 urls with 0.0 successes\n",
      "STATS For class all:\n",
      " tried 743.0 urls with 524.0 successes\n",
      "70.52489905787348% success rate for all urls \n",
      "0.07934677282362494 seconds spent per all succesful image download\n",
      "Scraping images for class \"plant\"\n",
      "Multiprocessing workers: 8\n",
      "\n",
      "Scraping stats:\n",
      "STATS For class is_flickr:\n",
      " tried 993.0 urls with 708.0 successes\n",
      "71.29909365558912% success rate for is_flickr urls \n",
      "0.09345716406396554 seconds spent per is_flickr succesful image download\n",
      "STATS For class not_flickr:\n",
      " tried 0.0 urls with 0.0 successes\n",
      "STATS For class all:\n",
      " tried 993.0 urls with 708.0 successes\n",
      "71.29909365558912% success rate for all urls \n",
      "0.09346493354624948 seconds spent per all succesful image download\n",
      "\n",
      "Scraping stats:\n",
      "STATS For class is_flickr:\n",
      " tried 1242.0 urls with 830.0 successes\n",
      "66.82769726247987% success rate for is_flickr urls \n",
      "0.0903245449066162 seconds spent per is_flickr succesful image download\n",
      "STATS For class not_flickr:\n",
      " tried 0.0 urls with 0.0 successes\n",
      "STATS For class all:\n",
      " tried 1242.0 urls with 830.0 successes\n",
      "66.82769726247987% success rate for all urls \n",
      "0.0903313082384776 seconds spent per all succesful image download\n",
      "\n",
      "Scraping stats:\n",
      "STATS For class is_flickr:\n",
      " tried 1492.0 urls with 961.0 successes\n",
      "64.41018766756032% success rate for is_flickr urls \n",
      "0.08981854287940431 seconds spent per is_flickr succesful image download\n",
      "STATS For class not_flickr:\n",
      " tried 0.0 urls with 0.0 successes\n",
      "STATS For class all:\n",
      " tried 1492.0 urls with 961.0 successes\n",
      "64.41018766756032% success rate for all urls \n",
      "0.08982479088512345 seconds spent per all succesful image download\n",
      "\n",
      "Scraping stats:\n",
      "STATS For class is_flickr:\n",
      " tried 1743.0 urls with 1086.0 successes\n",
      "62.306368330464714% success rate for is_flickr urls \n",
      "0.08963157355408996 seconds spent per is_flickr succesful image download\n",
      "STATS For class not_flickr:\n",
      " tried 0.0 urls with 0.0 successes\n",
      "STATS For class all:\n",
      " tried 1743.0 urls with 1086.0 successes\n",
      "62.306368330464714% success rate for all urls \n",
      "0.08959219424983635 seconds spent per all succesful image download\n",
      "\n",
      "Scraping stats:\n",
      "STATS For class is_flickr:\n",
      " tried 1992.0 urls with 1219.0 successes\n",
      "61.19477911646587% success rate for is_flickr urls \n",
      "0.08905944401754327 seconds spent per is_flickr succesful image download\n",
      "STATS For class not_flickr:\n",
      " tried 0.0 urls with 0.0 successes\n",
      "STATS For class all:\n",
      " tried 1993.0 urls with 1219.0 successes\n",
      "61.16407425990968% success rate for all urls \n",
      "0.0890660814030235 seconds spent per all succesful image download\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='ImageNet image scraper')\n",
    "parser.add_argument('-scrape_only_flickr', default=True, type=lambda x: (str(x).lower() == 'true'))\n",
    "parser.add_argument('-number_of_classes', default = 2, type=int)\n",
    "parser.add_argument('-images_per_class', default = 1000, type=int)\n",
    "parser.add_argument('-data_root', default='', type=str)\n",
    "parser.add_argument('-use_class_list', default=True,type=lambda x: (str(x).lower() == 'true'))\n",
    "parser.add_argument('-class_list', default=['n00015388', 'n00017222'], nargs='*')\n",
    "parser.add_argument('-debug', default=False,type=lambda x: (str(x).lower() == 'true'))\n",
    "\n",
    "parser.add_argument('-multiprocessing_workers', default = 8, type=int)\n",
    "\n",
    "args, args_other = parser.parse_known_args()\n",
    "\n",
    "if args.debug:\n",
    "    logging.basicConfig(filename='imagenet_scarper.log', level=logging.DEBUG)\n",
    "\n",
    "if len(args.data_root) == 0:\n",
    "    logging.error(\"-data_root is required to run downloader!\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.isdir(args.data_root):\n",
    "    logging.error(f'folder {args.data_root} does not exist! please provide existing folder in -data_root arg!')\n",
    "    exit()\n",
    "\n",
    "\n",
    "IMAGENET_API_WNID_TO_URLS = lambda wnid: f'http://www.image-net.org/api/text/imagenet.synset.geturls?wnid={wnid}'\n",
    "\n",
    "current_folder =  os.path.dirname(os.path.realpath('ImageNet-downloader'))\n",
    "\n",
    "\n",
    "class_info_json_filename = 'imagenet_class_info.json'\n",
    "class_info_json_filepath = os.path.join(current_folder, class_info_json_filename)\n",
    "\n",
    "class_info_dict = dict()\n",
    "\n",
    "with open(class_info_json_filepath) as class_info_json_f:\n",
    "    class_info_dict = json.load(class_info_json_f)\n",
    "\n",
    "\n",
    "classes_to_scrape = []\n",
    "\n",
    "\n",
    "if args.use_class_list == True:\n",
    "   for item in args.class_list:\n",
    "       classes_to_scrape.append(item)\n",
    "       if item not in class_info_dict:\n",
    "           logging.error(f'Class {item} not found in ImageNete')\n",
    "           exit()\n",
    "\n",
    "elif args.use_class_list == False:\n",
    "    potential_class_pool = []\n",
    "    for key, val in class_info_dict.items():\n",
    "\n",
    "        if args.scrape_only_flickr:\n",
    "            if int(val['flickr_img_url_count']) * 0.9 > args.images_per_class:\n",
    "                potential_class_pool.append(key)\n",
    "        else:\n",
    "            if int(val['img_url_count']) * 0.8 > args.images_per_class:\n",
    "                potential_class_pool.append(key)\n",
    "\n",
    "    if (len(potential_class_pool) < args.number_of_classes):\n",
    "        logging.error(f\"With {args.images_per_class} images per class there are {len(potential_class_pool)} to choose from.\")\n",
    "        logging.error(f\"Decrease number of classes or decrease images per class.\")\n",
    "        exit()\n",
    "\n",
    "    picked_classes_idxes = np.random.choice(len(potential_class_pool), args.number_of_classes, replace = False)\n",
    "\n",
    "    for idx in picked_classes_idxes:\n",
    "        classes_to_scrape.append(potential_class_pool[idx])\n",
    "\n",
    "\n",
    "print(\"Picked the following clases:\")\n",
    "print([ class_info_dict[class_wnid]['class_name'] for class_wnid in classes_to_scrape ])\n",
    "\n",
    "imagenet_images_folder = os.path.join(args.data_root, 'data')\n",
    "if not os.path.isdir(imagenet_images_folder):\n",
    "    os.mkdir(imagenet_images_folder)\n",
    "\n",
    "\n",
    "scraping_stats = dict(\n",
    "    all=dict(\n",
    "        tried=0,\n",
    "        success=0,\n",
    "        time_spent=0,\n",
    "    ),\n",
    "    is_flickr=dict(\n",
    "        tried=0,\n",
    "        success=0,\n",
    "        time_spent=0,\n",
    "    ),\n",
    "    not_flickr=dict(\n",
    "        tried=0,\n",
    "        success=0,\n",
    "        time_spent=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# all methods\n",
    "def add_debug_csv_row(row):\n",
    "    with open('stats.csv', \"a\") as csv_f:\n",
    "        csv_writer = csv.writer(csv_f, delimiter=\",\")\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "class MultiStats():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.lock = Lock()\n",
    "\n",
    "        self.stats = dict(\n",
    "            all=dict(\n",
    "                tried=Value('d', 0),\n",
    "                success=Value('d',0),\n",
    "                time_spent=Value('d',0),\n",
    "            ),\n",
    "            is_flickr=dict(\n",
    "                tried=Value('d', 0),\n",
    "                success=Value('d',0),\n",
    "                time_spent=Value('d',0),\n",
    "            ),\n",
    "            not_flickr=dict(\n",
    "                tried=Value('d', 0),\n",
    "                success=Value('d', 0),\n",
    "                time_spent=Value('d', 0),\n",
    "            )\n",
    "        )\n",
    "    def inc(self, cls, stat, val):\n",
    "        with self.lock:\n",
    "            self.stats[cls][stat].value += val\n",
    "\n",
    "    def get(self, cls, stat):\n",
    "        with self.lock:\n",
    "            ret = self.stats[cls][stat].value\n",
    "        return ret\n",
    "\n",
    "multi_stats = MultiStats()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "if args.debug:\n",
    "    row = [\n",
    "        \"all_tried\",\n",
    "        \"all_success\",\n",
    "        \"all_time_spent\",\n",
    "        \"is_flickr_tried\",\n",
    "        \"is_flickr_success\",\n",
    "        \"is_flickr_time_spent\",\n",
    "        \"not_flickr_tried\",\n",
    "        \"not_flickr_success\",\n",
    "        \"not_flickr_time_spent\"\n",
    "    ]\n",
    "    add_debug_csv_row(row)\n",
    "\"\"\"\n",
    "\n",
    "def add_stats_to_debug_csv():\n",
    "    row = [\n",
    "        multi_stats.get('all', 'tried'),\n",
    "        multi_stats.get('all', 'success'),\n",
    "        multi_stats.get('all', 'time_spent'),\n",
    "        multi_stats.get('is_flickr', 'tried'),\n",
    "        multi_stats.get('is_flickr', 'success'),\n",
    "        multi_stats.get('is_flickr', 'time_spent'),\n",
    "        multi_stats.get('not_flickr', 'tried'),\n",
    "        multi_stats.get('not_flickr', 'success'),\n",
    "        multi_stats.get('not_flickr', 'time_spent'),\n",
    "    ]\n",
    "    add_debug_csv_row(row)\n",
    "\n",
    "def print_stats(cls, print_func):\n",
    "\n",
    "    actual_all_time_spent = time.time() - scraping_t_start.value\n",
    "    processes_all_time_spent = multi_stats.get('all', 'time_spent')\n",
    "\n",
    "    if processes_all_time_spent == 0:\n",
    "        actual_processes_ratio = 1.0\n",
    "    else:\n",
    "        actual_processes_ratio = actual_all_time_spent / processes_all_time_spent\n",
    "\n",
    "    #print(f\"actual all time: {actual_all_time_spent} proc all time {processes_all_time_spent}\")\n",
    "\n",
    "    print_func(f'STATS For class {cls}:')\n",
    "    print_func(f' tried {multi_stats.get(cls, \"tried\")} urls with'\n",
    "               f' {multi_stats.get(cls, \"success\")} successes')\n",
    "\n",
    "    if multi_stats.get(cls, \"tried\") > 0:\n",
    "        print_func(f'{100.0 * multi_stats.get(cls, \"success\")/multi_stats.get(cls, \"tried\")}% success rate for {cls} urls ')\n",
    "    if multi_stats.get(cls, \"success\") > 0:\n",
    "        print_func(f'{multi_stats.get(cls,\"time_spent\") * actual_processes_ratio / multi_stats.get(cls,\"success\")} seconds spent per {cls} succesful image download')\n",
    "\n",
    "\n",
    "\n",
    "lock = Lock()\n",
    "url_tries = Value('d', 0)\n",
    "scraping_t_start = Value('d', time.time())\n",
    "class_folder = ''\n",
    "class_images = Value('d', 0)\n",
    "\n",
    "def get_image(img_url):\n",
    "\n",
    "    #print(f'Processing {img_url}')\n",
    "\n",
    "    #time.sleep(3)\n",
    "\n",
    "    if len(img_url) <= 1:\n",
    "        return\n",
    "\n",
    "\n",
    "    cls_imgs = 0\n",
    "    with lock:\n",
    "        cls_imgs = class_images.value\n",
    "\n",
    "    if cls_imgs >= args.images_per_class:\n",
    "        return\n",
    "\n",
    "    logging.debug(img_url)\n",
    "\n",
    "    cls = ''\n",
    "\n",
    "    if 'flickr' in img_url:\n",
    "        cls = 'is_flickr'\n",
    "    else:\n",
    "        cls = 'not_flickr'\n",
    "        if args.scrape_only_flickr:\n",
    "            return\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    def finish(status):\n",
    "        t_spent = time.time() - t_start\n",
    "        multi_stats.inc(cls, 'time_spent', t_spent)\n",
    "        multi_stats.inc('all', 'time_spent', t_spent)\n",
    "\n",
    "        multi_stats.inc(cls,'tried', 1)\n",
    "        multi_stats.inc('all', 'tried', 1)\n",
    "\n",
    "        if status == 'success':\n",
    "            multi_stats.inc(cls,'success', 1)\n",
    "            multi_stats.inc('all', 'success', 1)\n",
    "\n",
    "        elif status == 'failure':\n",
    "            pass\n",
    "        else:\n",
    "            logging.error(f'No such status {status}!!')\n",
    "            exit()\n",
    "        return\n",
    "\n",
    "\n",
    "    with lock:\n",
    "        url_tries.value += 1\n",
    "        if url_tries.value % 250 == 0:\n",
    "            print(f'\\nScraping stats:')\n",
    "            print_stats('is_flickr', print)\n",
    "            print_stats('not_flickr', print)\n",
    "            print_stats('all', print)\n",
    "            if args.debug:\n",
    "                add_stats_to_debug_csv()\n",
    "\n",
    "    try:\n",
    "        img_resp = requests.get(img_url, timeout = 1)\n",
    "    except ConnectionError:\n",
    "        logging.debug(f\"Connection Error for url {img_url}\")\n",
    "        return finish('failure')\n",
    "    except ReadTimeout:\n",
    "        logging.debug(f\"Read Timeout for url {img_url}\")\n",
    "        return finish('failure')\n",
    "    except TooManyRedirects:\n",
    "        logging.debug(f\"Too many redirects {img_url}\")\n",
    "        return finish('failure')\n",
    "    except MissingSchema:\n",
    "        return finish('failure')\n",
    "    except InvalidURL:\n",
    "        return finish('failure')\n",
    "\n",
    "    if not 'content-type' in img_resp.headers:\n",
    "        return finish('failure')\n",
    "\n",
    "    if not 'image' in img_resp.headers['content-type']:\n",
    "        logging.debug(\"Not an image\")\n",
    "        return finish('failure')\n",
    "\n",
    "    if (len(img_resp.content) < 1000):\n",
    "        return finish('failure')\n",
    "\n",
    "    logging.debug(img_resp.headers['content-type'])\n",
    "    logging.debug(f'image size {len(img_resp.content)}')\n",
    "\n",
    "    img_name = img_url.split('/')[-1]\n",
    "    img_name = img_name.split(\"?\")[0]\n",
    "\n",
    "    if (len(img_name) <= 1):\n",
    "        return finish('failure')\n",
    "\n",
    "    img_file_path = os.path.join(class_folder, img_name)\n",
    "    logging.debug(f'Saving image in {img_file_path}')\n",
    "\n",
    "    with open(img_file_path, 'wb') as img_f:\n",
    "        img_f.write(img_resp.content)\n",
    "\n",
    "        with lock:\n",
    "            class_images.value += 1\n",
    "\n",
    "        logging.debug(f'Scraping stats')\n",
    "        print_stats('is_flickr', logging.debug)\n",
    "        print_stats('not_flickr', logging.debug)\n",
    "        print_stats('all', logging.debug)\n",
    "\n",
    "        return finish('success')\n",
    "\n",
    "\n",
    "for class_wnid in classes_to_scrape:\n",
    "\n",
    "    class_name = class_info_dict[class_wnid][\"class_name\"]\n",
    "    print(f'Scraping images for class \\\"{class_name}\\\"')\n",
    "    url_urls = IMAGENET_API_WNID_TO_URLS(class_wnid)\n",
    "\n",
    "    time.sleep(0.05)\n",
    "    resp = requests.get(url_urls)\n",
    "\n",
    "    class_folder = os.path.join(imagenet_images_folder, class_name)\n",
    "    if not os.path.exists(class_folder):\n",
    "        os.mkdir(class_folder)\n",
    "\n",
    "    class_images.value = 0\n",
    "\n",
    "    urls = [url.decode('utf-8') for url in resp.content.splitlines()]\n",
    "\n",
    "    #for url in  urls:\n",
    "    #    get_image(url)\n",
    "\n",
    "    print(f\"Multiprocessing workers: {args.multiprocessing_workers}\")\n",
    "    with Pool(processes=args.multiprocessing_workers) as p:\n",
    "        p.map(get_image,urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
